{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Describe a data project you worked on recently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of my favorite project that I did as part of my Data Analyst Nanodegree was on Email Fraud detection where I used Machine Learning and Python. We had some data about the Enron company emails and financial information of the employees, the goal was to analyze the data and identify employees who committed the fraud. I used Python and Pandas to analyze imbalance in the distinctive features, imputed the null values and detected anomalies. Then I performed exploratory data analysis and created a lot of visualizations to see correlation among the features, this process helped in feature selection as I wanted to capture trends in the data, I did not use features that do not give any information. So to benchmark, I implemented few classifiers like Naïve Bayes, KNearest Neighbors, Random Forest and Support Vector Machines with default settings. Naïve Bayes, KNN and RandomForest performed poorly so I rejected those and chose SVM as my baseline model. Then I did feature engineering and created two new features which described the number of emails sent and received by each employee, and salary vs other means of income ratio. I also performed hyper parameter optimization using sklearn’s grid search cv on 1000 epoch cycles and tuned by SVM to rbf kernel and optimal C value which gave me accuracy over 90% on the test data, and 0.88 and 0.92 recall rates. In conclusion, I found out that people who committed fraud had way more income than their salary, the people from they received and sent email the most were also involved in the fraud. Some of the challenges I faced was in tuning SVM, so I had to look up online and update by knowledge.\n",
    "\n",
    "I also used Excel and created some pivot tables on the entire dataset to verify if my analysis was correct. Overall, I enjoyed working on a real-world dataset and I feel someone can take this project further by engineering more features of importance or use deep learning algorithms to identify the email fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: You are given a ten piece box of chocolate truffles. You know based on the label that six of the pieces have an orange cream filling and four of the pieces have a coconut filling. If you were to eat four pieces in a row, what is the probability that the first two pieces you eat have an orange cream filling and the last two have a coconut filling?\n",
    "\n",
    "#### Follow-up question: If you were given an identical box of chocolates and again eat four pieces in a row, what is the probability that exactly two contain coconut filling?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Given the table users:\n",
    "\n",
    "Table `users`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "| Column      | Type      |\n",
    "|-------------|-----------|\n",
    "| id          | integer   |\n",
    "| username    | character |\n",
    "| email       | character |\n",
    "| city        | character |\n",
    "| state       | character |\n",
    "| zip         | integer   |\n",
    "| active      | boolean   |\n",
    "\n",
    "construct a query to find the top 5 states with the highest number of active users. Include the number for each state in the query result. Example result:\n",
    "\n",
    "| state      | num_active_users |\n",
    "|------------|------------------|\n",
    "| New Mexico | 502              |\n",
    "| Alabama    | 495              |\n",
    "| California | 300              |\n",
    "| Maine      | 201              |\n",
    "| Texas      | 189              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "SELECT state, count(id) as num_active_users\n",
    "FROM users\n",
    "WHERE active=’True’\n",
    "GROUP BY state\n",
    "ORDER BY num_active_users DESC\n",
    "LIMIT 5;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Define a function first_unique that takes a string as input and returns the first non-repeated (unique) character in the input string. If there are no unique characters return None. Note: Your code should be in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "a\n",
      "None\n",
      "b\n",
      "j\n"
     ]
    }
   ],
   "source": [
    "#Python3\n",
    "#@author: jubinsoni\n",
    "import unittest\n",
    "\n",
    "def first_unique(string):\n",
    "    '''Returns first unique character of the string'''\n",
    "    if len(string) == 0:\n",
    "        return None\n",
    "    if len(string) == 1:\n",
    "        return string\n",
    "    \n",
    "    unique_dict = {}\n",
    "    for s in string:\n",
    "        if s not in unique_dict:\n",
    "            unique_dict[s] = 1\n",
    "        else:\n",
    "            unique_dict[s] += 1\n",
    "            \n",
    "    unique_char_list = [key for key, value in unique_dict.items() if value == 1]\n",
    "    unique_char = None\n",
    "    \n",
    "    if len(unique_char_list) == 0:\n",
    "        return unique_char\n",
    "    elif len(unique_char_list) == 1:\n",
    "        unique_char = unique_char_list[0]\n",
    "        return unique_char\n",
    "    else:\n",
    "        smallest_idx_char = -1\n",
    "        for u in unique_char_list:\n",
    "            if smallest_idx_char > string.index(u) or smallest_idx_char==-1:\n",
    "                smallest_idx_char = string.index(u)\n",
    "            else:\n",
    "                continue\n",
    "        unique_char = string[smallest_idx_char]\n",
    "    return unique_char\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    #Testcase1: Expected 'c'\n",
    "    print(first_unique('aabbcdd123'))\n",
    "    \n",
    "    #Testcase2: Expected 'a'\n",
    "    print(first_unique('a'))\n",
    "    \n",
    "    #Testcase3: Expected None\n",
    "    print(first_unique('112233'))\n",
    "    \n",
    "    #Testcase4(Edge): Expected b\n",
    "    print(first_unique('bcdefghijklmnopqrstuv'))\n",
    "    \n",
    "    #Testcase5(Edge): Expected j\n",
    "    print(first_unique('bbccddeeffgghhiijkkllmmnnoppqqrrssttuuvvxxyyxzz1'))\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
