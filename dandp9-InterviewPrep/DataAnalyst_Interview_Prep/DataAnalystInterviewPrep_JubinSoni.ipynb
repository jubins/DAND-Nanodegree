{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analyst Interview Prep Project\n",
    "\n",
    "- Author: Jubin Soni\n",
    "- Udacity Data Analyst Nanodegree Project\n",
    "- <a href='https://htmlpreview.github.io/?https://github.com/jubins/DAND-Nanodegree/blob/master/dandp9-InterviewPrep/DataAnalyst_Interview_Prep/DataAnalystInterviewPrep_JubinSoni.html'>Github Link</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1: Describe a data project you worked on recently.\n",
    "\n",
    "One of my favorite project that I did as part of my Data Analyst Nanodegree was on Email Fraud detection where I used Machine Learning and Python. We had some data about the Enron company emails and financial information of the employees, the goal was to analyze the data and identify employees who committed the fraud. I used Python and Pandas to analyze imbalance in the distinctive features, imputed the null values and detected anomalies. Then I performed exploratory data analysis and created a lot of visualizations to see correlation among the features, this process helped in feature selection as I wanted to capture trends in the data, I did not use features that do not give any information. So to benchmark, I implemented few classifiers like Naïve Bayes, KNearest Neighbors, Random Forest and Support Vector Machines with default settings. Naïve Bayes, KNN and RandomForest performed poorly so I rejected those and chose SVM as my baseline model. Then I did feature engineering and created two new features which described the number of emails sent and received by each employee, and salary vs other means of income ratio. I also performed hyper parameter optimization using sklearn’s grid search cv on 1000 epoch cycles and tuned by SVM to rbf kernel and optimal C value which gave me accuracy over 90% on the test data, and 0.88 and 0.92 recall rates. In conclusion, I found out that people who committed fraud had way more income than their salary, the people from they received and sent email the most were also involved in the fraud. Some of the challenges I faced was in tuning SVM, so I had to look up online and update by knowledge.\n",
    "\n",
    "I also used Excel and created some pivot tables on the entire dataset to verify if my analysis was correct. Overall, I enjoyed working on a real-world dataset and I feel someone can take this project further by engineering more features of importance or use deep learning algorithms to identify the email fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 2: You are given a ten piece box of chocolate truffles. You know based on the label that six of the pieces have an orange cream filling and four of the pieces have a coconut filling. If you were to eat four pieces in a row, what is the probability that the first two pieces you eat have an orange cream filling and the last two have a coconut filling?\n",
    "\n",
    "- We have 6 orange cream chocolate truffles and 4 coconut chocolate truffles. The probability of selecting an orange cream chocolate truffles at this time is 6/10.\n",
    "\n",
    "- Then we will have 5 orange cream chocolate truffles and 4 coconut chocolate truffles left, so the probability of selecting an orange cream chocolate truffles at this time is 5/9.\n",
    "\n",
    "- Now, we have 4 orange cream chocolate truffles and 4 chocolate truffles left, so the probability of selecting a coconut filling is 4/8.\n",
    "\n",
    "- So now we have 4 orange cream chocolate truffles and 3 coconut chocolate truffles. The probability of selecting a coconut chocolate truffle is 3/7.\n",
    "\n",
    "- Because the actions are related (truffles are taken one by one), the probability that the first two pieces you eat have an orange cream filling and the last two have a coconut filling is:\n",
    "\n",
    "```math\n",
    "(6/10) * (5/9) * (4/8) * (3/7) = 0.0714\n",
    "```\n",
    "\n",
    "#### Follow-up question: If you were given an identical box of chocolates and again eat four pieces in a row, what is the probability that exactly two contain coconut filling?\n",
    "\n",
    "- We have 14 combinations available:\n",
    "\n",
    "```str\n",
    "CCCC, CCCO, CCOO, COOO\n",
    "OOOO, OOOC, OOCC, OOOC\n",
    "COOC, OCCO ,COCO, OCOC\n",
    "OCOO, COCC\n",
    "```\n",
    "\n",
    "- To eat exacly two coconut truffles we have 6 combinations available out of total 14 combinations:\n",
    "\n",
    "```str\n",
    "CCOO, COCO, COOC,\n",
    "OOCC, OCOC, OCCO\n",
    "```\n",
    "\n",
    "- We can add our calculated probability 0.0714 six times:\n",
    "\n",
    "```math\n",
    "6 * 0.0714 = 0.4284\n",
    "OR\n",
    "6/14 = 0.4284\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3: Given the table `users`: Construct a query to find the top 5 states with the highest number of active users. Include the number for each state in the example result.\n",
    "\n",
    "Table `users`\n",
    "\n",
    "```\n",
    "| Column      | Type      |\n",
    "|-------------|-----------|\n",
    "| id          | integer   |\n",
    "| username    | character |\n",
    "| email       | character |\n",
    "| city        | character |\n",
    "| state       | character |\n",
    "| zip         | integer   |\n",
    "| active      | boolean   |\n",
    "```\n",
    "\n",
    "```\n",
    "Example result:\n",
    "\n",
    "| state      | num_active_users |\n",
    "|------------|------------------|\n",
    "| New Mexico | 502              |\n",
    "| Alabama    | 495              |\n",
    "| California | 300              |\n",
    "| Maine      | 201              |\n",
    "| Texas      | 189              |\n",
    "```\n",
    "\n",
    "\n",
    "```sql\n",
    "SELECT state, count(id) as num_active_users\n",
    "FROM users\n",
    "WHERE active=True\n",
    "GROUP BY state\n",
    "ORDER BY num_active_users DESC\n",
    "LIMIT 5;\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4: Define a function first_unique that takes a string as input and returns the first non-repeated (unique) character in the input string. If there are no unique characters return None. Note: Your code should be in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c\n",
      "a\n",
      "None\n",
      "b\n",
      "j\n"
     ]
    }
   ],
   "source": [
    "#Python3\n",
    "#@author: jubinsoni\n",
    "\n",
    "def first_unique(string):\n",
    "    '''Returns first unique character of the string'''\n",
    "    if len(string) == 0:\n",
    "        return None\n",
    "    if len(string) == 1:\n",
    "        return string\n",
    "    \n",
    "    unique_dict = {}\n",
    "    for s in string:\n",
    "        if s not in unique_dict:\n",
    "            unique_dict[s] = 1\n",
    "        else:\n",
    "            unique_dict[s] += 1\n",
    "            \n",
    "    unique_char_list = [key for key, value in unique_dict.items() if value == 1]\n",
    "    unique_char = None\n",
    "    \n",
    "    if len(unique_char_list) == 0:\n",
    "        return unique_char\n",
    "    elif len(unique_char_list) == 1:\n",
    "        unique_char = unique_char_list[0]\n",
    "        return unique_char\n",
    "    else:\n",
    "        smallest_idx_char = -1\n",
    "        for u in unique_char_list:\n",
    "            if smallest_idx_char > string.index(u) or smallest_idx_char==-1:\n",
    "                smallest_idx_char = string.index(u)\n",
    "            else:\n",
    "                continue\n",
    "        unique_char = string[smallest_idx_char]\n",
    "    return unique_char\n",
    "    \n",
    "    \n",
    "def main():\n",
    "    #Testcase1: Expected 'c'\n",
    "    print(first_unique('aabbcdd123'))\n",
    "    \n",
    "    #Testcase2: Expected 'a'\n",
    "    print(first_unique('a'))\n",
    "    \n",
    "    #Testcase3: Expected None\n",
    "    print(first_unique('112233'))\n",
    "    \n",
    "    #Testcase4(Edge): Expected b\n",
    "    print(first_unique('bcdefghijklmnopqrstuv'))\n",
    "    \n",
    "    #Testcase5(Edge): Expected j\n",
    "    print(first_unique('bbccddeeffgghhiijkkllmmnnoppqqrrssttuuvvxxyyxzz1'))\n",
    "    \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5: What are underfitting and overfitting in the context of Machine Learning? How might you balance them?\n",
    "\n",
    "Underfitting is when we have high bias and low variance, which means our data set contains very few records or records that are not completely spread out i.e. majority of the records about a single label in the dataset. Underfitting affects model’s ability to clearly fit the training data. Overfitting occurs when we have low bias and high variance, which means the samples in dataset are totally scattered or too much noise in the dataset. Overfitting affects model’s ability to generalize well on unseen data. Both of these, problems affect the machine learning algorithms ability to predict results accurately. To avoid underfitting, we should get add more data into our dataset, get rid of redundant features by feature selection and create more relevant features by feature engineering. To avoid overfitting, we should use cross-validation (a good rule of thumb is having 60:20:20 ratio in training:cv:test data), use regularization that adds penalty term or use ensemble methods like Random Forest, Bagging or Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6: If you were to start your data analyst position today, what would be your goals a year from now?\n",
    "\n",
    "My immediate goal would be to learn as much as I can from my peers and become an active contributor in the team as quickly as possible. Then within six months I plan to deliver impactful insights, use Excel to interpret and display data and use VB scripts to automate routine jobs in Excel. I also plan to use my Python, Pandas, Hadoop, NoSQL and SQL knowledge for data acquisition, mining, extracting patterns on huge datasets and deliver business insights. Within one year, I want to develop expertise in my work and be one of the most contributing member in the team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Thank you."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
