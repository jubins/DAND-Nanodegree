{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# OpenStreetMap Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<li>Name: Jubin Soni</li>\n",
    "<li>Udacity DAND Nanodegree Project 3</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map Area\n",
    "New York, NY, United States\n",
    "<ul>\n",
    "<li>Data Source: https://www.openstreetmap.org/relation/175905</li>\n",
    "<li>This map is where I say so I am more interested in what this study reveals by querying the database. I would also like to contribute to its improvement on OpenStreetMap.org.</li>\n",
    "<li>The downloaded OSM file is 2.7GB in size and it is difficult to process so much data so first I have created a sample of 270MB from this file and used it for initial analysis.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a sample of original OSM file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Following code shortens the OSM_FILE and stores it in SAMEPLE_FILE by dividing factor of 'k'\n",
    "import xml.etree.ElementTree as ET  # Use cElementTree or lxml if too slow\n",
    "\n",
    "OSM_FILE = \"new-york.osm\"  # Replace this with your osm file\n",
    "SAMPLE_FILE = \"new-york-sample.osm\"\n",
    "\n",
    "k = 100 # Parameter: take every k-th top level element\n",
    "\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\n",
    "\n",
    "    Reference:\n",
    "    http://stackoverflow.com/questions/3095434/inserting-newlines-in-xml-file-generated-via-xml-etree-elementtree-in-python\n",
    "    \"\"\"\n",
    "    context = iter(ET.iterparse(osm_file, events=('start', 'end')))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "with open(SAMPLE_FILE, 'wb') as output:\n",
    "    output.write('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n')\n",
    "    output.write('<osm>\\n  ')\n",
    "\n",
    "    # Write every kth top level element\n",
    "    for i, element in enumerate(get_element(OSM_FILE)):\n",
    "        if i % k == 0:\n",
    "            output.write(ET.tostring(element, encoding='utf-8'))\n",
    "\n",
    "    output.write('</osm>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problems Encountered in the Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After downloading and creating sample from the NYC OSM data. I noticed few problems with the data.\n",
    "<ul>\n",
    "<li> Abbreviated street names (\"E 14th St\", \"Myrtle Ave\", \"Father Capodanno Blvd\", \"White Plains Rd\") </li>\n",
    "<li> Inconsistent zip codes (\"10307:10312\", \"11220; 11204\") </li>\n",
    "<li> Overabbreviated and Inconsistent name_types (\"Blvd;Blvd;Pl;Blvd\") </li>\n",
    "<li> Inorrect zip codes (\"07544\", \"06853\"). All zip codes in NYC start from 10 or 11 </li>\n",
    "<li> Street names in second level “k” tags pulled from Tiger GPS data and divided into segment</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting these problems (Audit.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def update(name, mapping): \n",
    "    words = name.split()\n",
    "    for w in range(len(words)):\n",
    "        if words[w] in mapping:\n",
    "            if words[w].lower() not in ['suite', 'ste.', 'ste']: \n",
    "                # For example, don't update 'Suite E' to 'Suite East'\n",
    "                words[w] = mapping[words[w]]\n",
    "                name = \" \".join(words)\n",
    "    return name\n",
    "\n",
    "def correct_abbreviations(problem_field, tree, mapping):\n",
    "    for tag in tree.iter('tag'):\n",
    "        if problem_field in tag.attrib['k']:\n",
    "            words = tag.attrib['v']\n",
    "            updated_words = \"\"\n",
    "            for word in words.split(\" \"):\n",
    "                if (word in mapping):\n",
    "                    word = mapping[word]\n",
    "                updated_words = \"\".join(word)\n",
    "            tag.attrib['v'] = updated_words\n",
    "\n",
    "\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "SAMPLE_FILE = \"new-york-sample.osm\"\n",
    "tree = ET.parse(SAMPLE_FILE)\n",
    "\n",
    "street_mapping = {\"St\":\"Street\", \"St.\":\"Street\", \"Ave\": \"Avenue\", \"ave\": \"Avenue\", \"Blvd\": \"Boulevard\", \"Rd\": \"Road\"}\n",
    "name_type_mapping = {\"Pkwy\": \"Parkway\", \"St\": \"Street\", \"St.\":\"Street\", \"Ave\": \"Avenue\", \"ave\": \"Avenue\", \"Blvd\": \"Boulevard\", \"Rd\": \"Road\"}\n",
    "\n",
    "correct_abbreviations('street', tree, street_mapping)\n",
    "correct_abbreviations('name_type', tree, name_type_mapping)\n",
    "tree.write(SAMPLE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning (Data.py): Processing the OSM file and storing in separate CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed OSM file and 5 new files have been created:\n",
      "- nodes.csv\n",
      "- nodes_tags.csv\n",
      "- ways.csv\n",
      "- ways_tags.csv\n",
      "- ways_nodes.csv\n"
     ]
    }
   ],
   "source": [
    "#data.py\n",
    "import csv\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "import cerberus\n",
    "import schema_\n",
    "\n",
    "OSM_PATH = SAMPLE_FILE #SAMPLE_FILE = \"new-york-sample.osm\"\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "SCHEMA = schema_.schema\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    if element.tag == 'node':\n",
    "        for field in NODE_FIELDS:\n",
    "            node_attribs[field] = element.attrib[field]\n",
    "        \n",
    "        for tag in element.iter('tag'):\n",
    "            tag_dict = {}\n",
    "            tag_dict['id'] = element.attrib['id']\n",
    "            if PROBLEMCHARS.match(tag.attrib['k']):\n",
    "                pass\n",
    "            elif ':' in tag.attrib['k']:\n",
    "                tag_splittable = tag.attrib['k']\n",
    "                tag_dict['type'] = tag_splittable.split(':')[0]\n",
    "                tag_dict['key'] = tag_splittable.split(':',1)[1]\n",
    "            else:\n",
    "                tag_dict['type'] = 'regular'\n",
    "                tag_dict['key'] = tag.attrib['k']\n",
    "                \n",
    "            tag_dict['value'] = tag.attrib['v']\n",
    "            tags.append(tag_dict)\n",
    "        \n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "            \n",
    "    if element.tag == 'way':\n",
    "        for field in WAY_FIELDS:\n",
    "            way_attribs[field] = element.attrib[field]\n",
    "        \n",
    "        position_counter = 0\n",
    "        for nd in element.iter('nd'):\n",
    "            nd_dict = {}\n",
    "            nd_dict['id'] = element.attrib['id']\n",
    "            nd_dict['node_id'] = nd.attrib['ref']\n",
    "            nd_dict['position'] = position_counter\n",
    "            position_counter += 1\n",
    "            way_nodes.append(nd_dict)\n",
    "            \n",
    "        for tag in element.iter('tag'):\n",
    "            tag_dict = {}\n",
    "            tag_dict['id'] = element.attrib['id']\n",
    "            if PROBLEMCHARS.match(tag.attrib['k']):\n",
    "                pass\n",
    "            elif ':' in tag.attrib['k']:\n",
    "                tag_dict['type'] = tag.attrib['k'].split(':')[0]\n",
    "                tag_dict['key'] = tag.attrib[\"k\"].split(':',1)[1]\n",
    "            else:\n",
    "                tag_dict['type'] = 'regular'\n",
    "                tag_dict['key'] = tag.attrib['k']\n",
    "            #value\n",
    "            tag_dict['value'] = tag.attrib['v']\n",
    "            tags.append(tag_dict)\n",
    "            \n",
    "    return {'way': way_attribs, 'way_tags': tags, 'way_nodes': way_nodes}\n",
    "\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    \"\"\"Raise ValidationError if element does not match schema\"\"\"\n",
    "    if validator.validate(element, SCHEMA) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string))\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, unicode) else v) for k, v in row.iteritems()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in, validate):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "         codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "         codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "        codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "         codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "\n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        validator = cerberus.Validator()\n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Note: Validation is ~ 10X slower. For the project consider using a small\n",
    "    # sample of the map when validating.\n",
    "    process_map(OSM_PATH, validate=True)\n",
    "    print (\"Processed OSM file and 5 new files have been created:\\n- nodes.csv\\n- nodes_tags.csv\\n- ways.csv\\n- ways_tags.csv\\n- ways_nodes.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SQLite3 (Data Exploration and Gathering insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The five files generated from above script were fed into SQLite3 and used to answer few questions about NYC Metro Area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "sqlite> select count(*) from nodes;\n",
    "\n",
    "Output: 230102\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "sqlite> select count(*) from ways;\n",
    "\n",
    "Output: 36066\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "sqlite> select count(distinct(e.uid)) from\n",
    "(select uid from nodes union all select uid from ways) e;\n",
    "\n",
    "Output: 1252\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 contributing users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "sqlite> select e.user, count(*) as num from\n",
    "(select user from nodes union all select user from ways) e\n",
    "group by e.user\n",
    "order by num desc\n",
    "limit 10;\n",
    "\n",
    "Output:\n",
    "\n",
    "|e.user               |num  |\n",
    "|---------------------|-----|\n",
    "|                     |48886|\n",
    "|Rub21_nycbuildings   |9347 |\n",
    "|ingalls_nycbuildings |6267 |\n",
    "|MySuffolkNY          |6186 |\n",
    "|woodpeck_fixbot      |5806 |\n",
    "|minewman             |4912 |\n",
    "|Northfork            |4140 |\n",
    "|ediyes_nycbuildings  |2711 |\n",
    "|lxbarth_nycbuildings |2350 |\n",
    "|---------------------|-----|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of users apprearing only once (having 1 post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "sqlite> select count(*) from\n",
    "(select e.user, count(*) as num from\n",
    "(select user from nodes union all select user from ways) e\n",
    "group by e.user\n",
    "having num = 1);\n",
    "\n",
    "Output: 56\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 appearing amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "sqlite> select value, count(*) as num\n",
    "from nodes_tags\n",
    "where key = 'amenity'\n",
    "goup by value\n",
    "order by num desc\n",
    "limit 10;\n",
    "\n",
    "Output:\n",
    "\n",
    "|value            |num  |\n",
    "|-----------------|-----|\n",
    "|bicycle parking  |51   |\n",
    "|restaurant       |33   |\n",
    "|school           |31   |\n",
    "|place_of_worship |13   |\n",
    "|fast_food        |15   |\n",
    "|cafe             |13   |\n",
    "|bar              |8    |\n",
    "|pharmacy         |6    |\n",
    "|bicycle_rental   |5    |\n",
    "|fire_station     |5    |\n",
    "|-----------------|-----|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 10 cuisines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```SQL\n",
    "sqlite> select nodes_tags.value, count(*) as num\n",
    "from nodes_tags join\n",
    "(select distinct(id) from nodes_tags where value = 'restaurant') r\n",
    "on nodes_tags.id = r.id\n",
    "where nodes_tags.key = 'cuisine'\n",
    "group by nodes_tags.value\n",
    "order by num desc;\n",
    "\n",
    "Output:\n",
    "\n",
    "|nodes_tags.value         |num  |\n",
    "|-------------------------|-----|\n",
    "|american                 |2    |\n",
    "|Mexican                  |2    |\n",
    "|american;regional;burger |1    |\n",
    "|asian                    |1    |\n",
    "|chinese                  |1    |\n",
    "|diner                    |1    |\n",
    "|french                   |1    |\n",
    "|frozen yogurt            |1    |\n",
    "|ice_cream                |1    |\n",
    "|japanese                 |1    |\n",
    "|korean                   |1    |\n",
    "|mexican                  |1    |\n",
    "|pizza                    |1    |\n",
    "|-------------------------|-----|\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future prospects and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this review of the data it is obvious that the New York City area is very complete, and although I believe it has been well cleaned for the purposes of this exercise I think further cleaning the data will reveal more interesting insights. For example in the 'Top 10 Cuisines' one of the nodes_tags.value has multiple comma-separated value and in 'Top 10 contributors' the first field is blank. This shows that in the entire file there can be more such values and these minor corrections can be done. Also some more complicated analysis can be done using SQL like most occuring (worshipped) religion in OSM, and most active user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It interests me to notice a fair amount of GPS data makes it into OpenStreetMap.org on account of users’ efforts, whether by scripting a map editing bot or otherwise. With a rough GPS data processor in place and working together with a more robust data processor similar to data.py and audit.py. I think it would be possible to input a great amount of cleaned, validated and corrected data to OpenStreetMap.org."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
